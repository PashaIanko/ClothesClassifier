{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion-Mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages, constants & functions"
      ],
      "metadata": {
        "id": "ewaMo6fvE0BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "utils = {\n",
        "    'GridProperties.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/GridProperties.py',\n",
        "    'GSModel.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/GSModel.py',\n",
        "    'GridSearchParameters.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/GridSearchParameters.py'\n",
        "}\n",
        "\n",
        "RF_utils = {\n",
        "    'RFGridProperties.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/RFModel/RFGridProperties.py',\n",
        "    'RFSearchParameters.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/RFModel/RFSearchParameters.py',\n",
        "    'RandomForestModel.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/RFModel/RandomForestModel.py'\n",
        "}\n",
        "\n",
        "SVM_utils = {\n",
        "    'SVMGridProperties.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/SVMModel/SVMGridProperties.py',\n",
        "    'SVMModel.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/SVMModel/SVMModel.py',\n",
        "    'SVMSearchParameters.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/SVMModel/SVMSearchParameters.py'\n",
        "}\n",
        "\n",
        "DT_utils = {\n",
        "    'DTGridProperties.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/DTModel/DTGridProperties.py',\n",
        "    'DTModel.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/DTModel/DTModel.py',\n",
        "    'DTSearchParameters.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/DTModel/DTSearchParameters.py'\n",
        "}\n",
        "\n",
        "XGB_utils = {\n",
        "    'XGBGridProperties.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/XGBModel/XGBGridProperties.py',\n",
        "    'XGBModel.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/XGBModel/XGBModel.py',\n",
        "    'XGBSearchParameters.py': 'https://raw.githubusercontent.com/PashaIanko/ClothesClassifier/main/XGBModel/XGBSearchParameters.py'\n",
        "}\n",
        "\n",
        "def download_files(url_dict):\n",
        "    for file, url in url_dict.items():\n",
        "        !wget -O {file} {url} {file}\n",
        "\n",
        "\n",
        "download_files(utils)\n",
        "download_files(RF_utils)\n",
        "download_files(SVM_utils)\n",
        "download_files(DT_utils)\n",
        "download_files(XGB_utils)"
      ],
      "metadata": {
        "id": "Xsqz-hivfxBb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProperties:\n",
        "    num_classes = 10\n",
        "    img_width = 28\n",
        "    img_height = 28\n",
        "    classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    translation = {\n",
        "        0: 'T-shirt',\n",
        "        1: 'Trouser', \n",
        "        2: 'Pullover',\n",
        "        3: 'Dress',\n",
        "        4: 'Coat',\n",
        "        5: 'Sandal', \n",
        "        6: 'Shirt',\n",
        "        7: 'Sneaker',\n",
        "        8: 'Bag',\n",
        "        9: 'Ankle boot'\n",
        "    }\n",
        "    seed = 12\n",
        "    subset_size = 5500\n",
        "    learning_curve_subset_size = 1000\n",
        "\n",
        "    def translate_labels(labels):\n",
        "        return [DataProperties.translation[num] for num in labels]\n",
        "    \n",
        "    def translate_label(class_num):\n",
        "        return DataProperties.translation[class_num]"
      ],
      "metadata": {
        "id": "wpfvMZXMKTr0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "!pip install nose"
      ],
      "metadata": {
        "id": "XWFNAL-rG0Ad"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data download & utils\n",
        "import numpy as np\n",
        "from urllib.request import urlopen\n",
        "from skimage import filters\n",
        "import pandas as pd\n",
        "\n",
        "# Testing\n",
        "from nose.tools import assert_equal\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_recall_fscore_support as prec_rec_f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skimage.feature import hog\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# My classes\n",
        "from GridProperties import GridProperties\n",
        "from GSModel import GSModel\n",
        "from GridSearchParameters import GridSearchParameters\n",
        "\n",
        "# RandomForest model\n",
        "from RFGridProperties import RFGridProperties\n",
        "from RFSearchParameters import RFSearchParameters\n",
        "from RandomForestModel import RandomForestModel\n",
        "\n",
        "# SVMModel\n",
        "from SVMGridProperties import SVMGridProperties\n",
        "from SVMSearchParameters import SVMSearchParameters\n",
        "from SVMModel import SVMModel\n",
        "\n",
        "# DecisionTreeModel\n",
        "from DTGridProperties import DTGridProperties\n",
        "from DTSearchParameters import DTSearchParameters\n",
        "from DTModel import DTModel\n",
        "\n",
        "# GradientBoostingModel\n",
        "from XGBGridProperties import XGBGridProperties\n",
        "from XGBSearchParameters import XGBSearchParameters\n",
        "from XGBModel import XGBModel\n",
        "\n",
        "# FFNN Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.python.framework.random_seed import set_random_seed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "CgCSZuPLE7Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490C2OoPs4PX"
      },
      "source": [
        "def load_fashion_mnist():\n",
        "    url_base = \"https://www.math.unipd.it/~dasan/\"\n",
        "    Y_train = np.frombuffer(urlopen(url_base + \"train-labels-idx1-ubyte\").read(), dtype=np.uint8, offset=8)\n",
        "    X_train = np.frombuffer(urlopen(url_base + \"train-images-idx3-ubyte\").read(), dtype=np.uint8, offset=16).reshape(len(Y_train), 784) # besides loadng \n",
        "                                                                                            #the data, I already flatten it into a vector\n",
        "    Y_test = np.frombuffer(urlopen(url_base + \"t10k-labels-idx1-ubyte\").read(), dtype=np.uint8, offset=8)\n",
        "    X_test = np.frombuffer(urlopen(url_base + \"t10k-images-idx3-ubyte\").read(), dtype=np.uint8, offset=16).reshape(len(Y_test), 784)\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class(class_n, X, Y, figsize = (10, 10), matsize = (3, 3)):\n",
        "    fig, axes = plt.subplots(matsize[0], matsize[1], figsize = figsize)\n",
        "    fig.suptitle(f'Image of instances of Class №{class_n} ({DataProperties.translate_label(class_n)})')\n",
        "    \n",
        "    class_instances = X[Y == class_n]\n",
        "    counter = 0\n",
        "    for i in range(matsize[0]):\n",
        "        for j in range(matsize[1]):\n",
        "            axes[i, j].imshow(\n",
        "                class_instances[counter].reshape(\n",
        "                    DataProperties.img_height, \n",
        "                    DataProperties.img_width\n",
        "                )\n",
        "            )\n",
        "            counter += 1"
      ],
      "metadata": {
        "id": "69qZZZC3O8ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_pixel_dist(X, Y, classes, sample_size):\n",
        "    fig = plt.figure(figsize = (8, 5))\n",
        "    for c in classes:\n",
        "        sample = X[Y == c][:sample_size]\n",
        "        sns.distplot(\n",
        "            sample, \n",
        "            hist = False, \n",
        "            label = f'Class {c}: {DataProperties.translate_label(c)}')\n",
        "    plt.title('Pixel greyscale value distribution')\n",
        "    fig.legend()"
      ],
      "metadata": {
        "id": "Sc6mxwgBWEjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reshaper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Reshape from linear to \n",
        "    square image\n",
        "    \"\"\"\n",
        " \n",
        "    def __init__(self):\n",
        "        pass\n",
        " \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"returns itself\"\"\"\n",
        "        return self\n",
        " \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"perform the transformation and return an array\"\"\"\n",
        "        return np.array(X).reshape(X.shape[0], DataProperties.img_height, DataProperties.img_width)"
      ],
      "metadata": {
        "id": "TzvT9Rmnd--6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatter(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Flat an image from 3D to 2D\n",
        "    \"\"\"\n",
        " \n",
        "    def __init__(self):\n",
        "        pass\n",
        " \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"returns itself\"\"\"\n",
        "        return self\n",
        " \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"perform the transformation and return an array\"\"\"\n",
        "        return np.array(X).reshape(len(X), -1)"
      ],
      "metadata": {
        "id": "wZP9QRkeEY3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EdgeHighlighter(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Highlight edges\n",
        "    \"\"\"\n",
        " \n",
        "    def __init__(self, add_coef):\n",
        "        self.add_coef = add_coef\n",
        " \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"returns itself\"\"\"\n",
        "        return self\n",
        " \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"perform the transformation and return an array\"\"\"\n",
        "        edges = filters.sobel(X)\n",
        "        return X + self.add_coef * edges"
      ],
      "metadata": {
        "id": "fI_L1gOlJOBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HogTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Expects an array of 2d arrays (1 channel images)\n",
        "    Calculates hog features for each img\n",
        "    \"\"\"\n",
        " \n",
        "    def __init__(self, y=None, orientations=8,\n",
        "                 pixels_per_cell=(2, 2),\n",
        "                 cells_per_block=(1, 1), block_norm='L2-Hys'):\n",
        "        self.y = y\n",
        "        self.orientations = orientations\n",
        "        self.pixels_per_cell = pixels_per_cell\n",
        "        self.cells_per_block = cells_per_block\n",
        "        self.block_norm = block_norm\n",
        " \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        " \n",
        "    def transform(self, X, y=None):\n",
        " \n",
        "        def local_hog(X):\n",
        "            return hog(X,\n",
        "                       orientations=self.orientations,\n",
        "                       pixels_per_cell=self.pixels_per_cell,\n",
        "                       cells_per_block=self.cells_per_block,\n",
        "                       block_norm=self.block_norm)\n",
        " \n",
        "        try: # parallel\n",
        "            return np.array([local_hog(img) for img in X])\n",
        "        except:\n",
        "            return np.array([local_hog(img) for img in X])"
      ],
      "metadata": {
        "id": "rl_fHPywEVUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_scores(cross_val_results):\n",
        "    res = {}\n",
        "    for name, cv_res in cross_val_results.items():\n",
        "        res[name] = cv_res.best_score_\n",
        "    return {k: v for k, v in sorted(res.items(), key = lambda item: item[1])}\n",
        "\n",
        "def plot_best_scores(scores_dict, y_label, title):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    ax.set_ylabel(y_label)\n",
        "    ax.set_title(title)\n",
        "\n",
        "    x_vals = best_scores.keys()\n",
        "    y_vals = best_scores.values()\n",
        "    \n",
        "    ax.bar(x_vals, y_vals)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hNOeQ03qTynC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cv_metrics(cross_val_res, metrics_name, figsize, layout_shape):\n",
        "    n_models = len(cross_val_res)\n",
        "    n_rows_cols = int(np.sqrt(n_models))\n",
        "    \n",
        "    fig, axes = plt.subplots(\n",
        "        layout_shape[0],\n",
        "        layout_shape[1],\n",
        "        sharex = False,\n",
        "        sharey = False,\n",
        "        figsize = figsize\n",
        "    )\n",
        "\n",
        "    counter = 0\n",
        "    for model_name, res in cross_val_res.items():\n",
        "        row = counter % n_rows_cols\n",
        "        col = counter // n_rows_cols\n",
        "        \n",
        "        if n_rows_cols == 1:\n",
        "            ax = axes[counter]\n",
        "        else:\n",
        "            ax = axes[row, col]\n",
        "\n",
        "        model_res = cross_val_res[model_name].cv_results_\n",
        "        train_res = model_res['mean_train_' + metrics_name]\n",
        "        test_res = model_res['mean_test_' + metrics_name]\n",
        "        cv_epochs = np.arange(1, len(train_res) + 1)\n",
        "\n",
        "        ax.bar(cv_epochs, train_res)\n",
        "        ax.bar(cv_epochs, test_res)\n",
        "        ax.set_title(f'CV results for {model_name}: {metrics_name}')\n",
        "        ax.set_xlabel('№ CV step')\n",
        "        ax.set_ylabel(f'Metrics: {metrics_name}')\n",
        "\n",
        "        ax.legend(['Train', 'Test'])\n",
        "\n",
        "        counter += 1"
      ],
      "metadata": {
        "id": "z1Lv4ECrgj0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(model, model_name, X_test, Y_test, figsize):\n",
        "    fig, axes = plt.subplots(1, 1, figsize = figsize)\n",
        "\n",
        "    ConfusionMatrixDisplay.from_estimator(\n",
        "        model,\n",
        "        X_test,\n",
        "        Y_test,\n",
        "        xticks_rotation = 'vertical',\n",
        "        display_labels = DataProperties.translate_labels(model.classes_),\n",
        "        cmap = 'Oranges',\n",
        "        normalize = 'true',\n",
        "        ax = axes\n",
        "    )\n",
        "    plt.title(f'Normalized confusion matrix for {model_name}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7XZw1lNdnuXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(model, model_name, X, Y, train_sizes, cv, scoring, metrics_name):\n",
        "    train_sizes, train_scores, validation_scores = learning_curve(\n",
        "        estimator = model,\n",
        "        X = X,\n",
        "        y = Y,\n",
        "        train_sizes = train_sizes,\n",
        "        cv = cv,\n",
        "        scoring = scoring\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.plot(\n",
        "        train_sizes, \n",
        "        train_scores.mean(axis=1), \n",
        "        label=f'{metrics_name}: Mean CV train scores',\n",
        "        linestyle = 'dashed',\n",
        "        marker = 'o',\n",
        "        markerfacecolor = 'white',\n",
        "        markersize = 9\n",
        "    )\n",
        "    plt.plot(\n",
        "        train_sizes, \n",
        "        validation_scores.mean(axis=1), \n",
        "        label=f'{metrics_name}: Mean CV validation scores',\n",
        "        marker = 'o',\n",
        "        markerfacecolor = 'white',\n",
        "        markersize = 9\n",
        "    )\n",
        "    plt.ylabel(f'{metrics_name} scores')\n",
        "    plt.xlabel('Train size')\n",
        "    plt.grid()\n",
        "    plt.title(f'Learning curve for {model_name}')\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "krTDTiy-7rpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_nn_learning_curve(history, metrics_name, val_metrics_name):\n",
        "    plt.plot(\n",
        "        history[metrics_name],\n",
        "        marker = 'o',\n",
        "        markerfacecolor = 'white',\n",
        "        markersize = 7\n",
        "    )\n",
        "\n",
        "    plt.plot(\n",
        "        history[val_metrics_name],\n",
        "        marker = 'o',\n",
        "        markerfacecolor = 'white',\n",
        "        markersize = 7\n",
        "    )\n",
        "\n",
        "    plt.title(f'FFNN model, {metrics_name} metrics')\n",
        "    plt.ylabel(metrics_name)\n",
        "    plt.xlabel('Epoch number')\n",
        "    plt.grid()\n",
        "    plt.legend(['Train', 'Validation'], loc = 'best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "s8mYwwa8OZ3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mistaken_images(true_label, mistaken_label, mistakes_percent, X, Y_true, Y_pred, model_name, layout_size = 2):\n",
        "    indices = np.all(\n",
        "        [\n",
        "         (Y_true == true_label),\n",
        "         (Y_pred == mistaken_label)\n",
        "        ],\n",
        "        axis = 0\n",
        "    )\n",
        "\n",
        "    _, axes = plt.subplots(\n",
        "        nrows = 1, \n",
        "        ncols = layout_size, \n",
        "        figsize = (10, 3)\n",
        "    )\n",
        "    for ax, image, label, pred_label in zip(axes, X[indices][: layout_size], Y_true[indices][: layout_size], Y_pred[indices][: layout_size]):\n",
        "        ax.set_axis_off()\n",
        "        ax.imshow(\n",
        "            image.reshape(DataProperties.img_height, DataProperties.img_width), \n",
        "            cmap=plt.cm.gray_r, \n",
        "            interpolation=\"nearest\"\n",
        "        )\n",
        "        ax.set_title(f\"\"\"\n",
        "            Model name: {model_name}\n",
        "            True label: {true_label} ({DataProperties.translate_label(true_label)})\n",
        "            Mistaken label: {mistaken_label} ({DataProperties.translate_label(mistaken_label)})\n",
        "            Mistakes %: {np.round(mistakes_percent * 100, 2)}\"\"\"\n",
        "        )"
      ],
      "metadata": {
        "id": "bIEuJWoKdT58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mistakes(best_estimator, model_name, X, Y_true, Y_predicted, top_n):\n",
        "    conf_mat = confusion_matrix(\n",
        "        Y_true,\n",
        "        Y_predicted,\n",
        "        labels = best_estimator.classes_,\n",
        "        normalize = 'true'\n",
        "    )\n",
        "    \n",
        "    for label in DataProperties.classes:\n",
        "        true_label = label\n",
        "        predicted_labels = conf_mat[true_label]\n",
        "\n",
        "        top_mistaken_labels = np.argsort(predicted_labels)[::-1][1 : top_n + 1]\n",
        "        top_percentages = np.sort(predicted_labels)[::-1][1 : top_n + 1]\n",
        "    \n",
        "        for i in range(len(top_mistaken_labels)):\n",
        "            mistaken_label = top_mistaken_labels[i]\n",
        "            mistakes_percent = top_percentages[i]\n",
        "            plot_mistaken_images(\n",
        "                true_label, \n",
        "                mistaken_label, \n",
        "                mistakes_percent,\n",
        "                X, \n",
        "                Y_true, \n",
        "                Y_predicted,\n",
        "                model_name\n",
        "            )"
      ],
      "metadata": {
        "id": "ufo_6pOydNpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_classif_reports(estimators_dict, X, Y):\n",
        "    res = {}\n",
        "    for name, estimator in estimators_dict.items():\n",
        "        if name == 'NeuralNetwork':\n",
        "            pred = estimator.predict(X).argmax(axis = 1)\n",
        "        else:\n",
        "            pred = estimator.predict(X)\n",
        "        precision, recall, fscore, _ = prec_rec_f1_score(\n",
        "            Y,\n",
        "            pred,\n",
        "            average = 'macro'  # Because dataset is balanced\n",
        "        )\n",
        "        accuracy = accuracy_score(Y, pred)\n",
        "        res[name] = {\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'Accuracy': accuracy,\n",
        "            'F1': fscore\n",
        "        }\n",
        "    precisions = {name: item['Precision'] for name, item in res.items()}\n",
        "    recalls = {name: item['Recall'] for name, item in res.items()}\n",
        "    accuracies = {name: item['Accuracy'] for name, item in res.items()}\n",
        "    F1 = {name: item['F1'] for name, item in res.items()}\n",
        "    return precisions, recalls, accuracies, F1"
      ],
      "metadata": {
        "id": "mZypeN8jHqDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_nn_model(input_dimension, learning_rate):\n",
        "    np.random.seed(DataProperties.seed)\n",
        "    set_random_seed(DataProperties.seed)\n",
        "\n",
        "    stopper = EarlyStopping(\n",
        "        monitor = 'val_accuracy',\n",
        "        mode = 'max',\n",
        "        patience = 2,\n",
        "        verbose = 1\n",
        "    )\n",
        "\n",
        "    Model = Sequential()\n",
        "    Model.add(Dense(input_dim = input_dimension, units = input_dimension, activation = 'relu'))\n",
        "    Model.add(Dense(units = 32, activation = 'relu'))\n",
        "    Model.add(Dense(units = DataProperties.num_classes, activation = 'softmax'))\n",
        "\n",
        "    Model.compile(\n",
        "        loss = 'categorical_crossentropy',\n",
        "        optimizer = SGD(learning_rate = learning_rate), \n",
        "        # optimizer = Adam(learning_rate = 0.001), - Bad learning curves\n",
        "        metrics = ['accuracy']\n",
        "    )\n",
        "    \n",
        "    return Model, stopper"
      ],
      "metadata": {
        "id": "6_Bl8nsRNWZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data download"
      ],
      "metadata": {
        "id": "YGrWQqESFLT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here, we split data into three sets:\n",
        "    - Train + validation\n",
        "    - Test\n",
        "- Test set remains untouched. Further, we split Train + validation set, in order to choose best model on unseen validation data. Best model results are reported on the unseen data from the test set."
      ],
      "metadata": {
        "id": "fOy4EDQNSSm-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPzMfoAKxhxQ"
      },
      "source": [
        "X_train_val, Y_train_val, X_test, Y_test = load_fashion_mnist()\n",
        "\n",
        "X_train_val = X_train_val[ : DataProperties.subset_size]\n",
        "Y_train_val = Y_train_val[ : DataProperties.subset_size]\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "    X_train_val,\n",
        "    Y_train_val,\n",
        "    stratify = Y_train_val,\n",
        "    train_size = 0.8\n",
        ")\n",
        "print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data clean check"
      ],
      "metadata": {
        "id": "whfHB7VPFQX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Make sure, that data is clean and does not contain missing pixels"
      ],
      "metadata": {
        "id": "SQzx_GHDJmlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check missing pixels\n",
        "def check_nan(arr):\n",
        "    assert_equal(np.isnan(arr).any(), False)\n",
        "\n",
        "check_nan(X_train)\n",
        "check_nan(X_val)\n",
        "check_nan(X_test)\n",
        "\n",
        "check_nan(Y_train)\n",
        "check_nan(Y_val)\n",
        "check_nan(Y_test)"
      ],
      "metadata": {
        "id": "CPB1CT7FE_2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data exploration"
      ],
      "metadata": {
        "id": "IwIc147HHGaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class presence in Train & Test"
      ],
      "metadata": {
        "id": "vQZoUOb5HK67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Make sure all range of classes is present in all datasets"
      ],
      "metadata": {
        "id": "Uj9qO6cPxaEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_vals = np.unique(Y_train)\n",
        "val_vals = np.unique(Y_val)\n",
        "test_vals = np.unique(Y_test)\n",
        "assert_equal((train_vals == test_vals).any(), True)\n",
        "assert_equal((train_vals == val_vals).any(), True)\n",
        "assert_equal(len(train_vals), DataProperties.num_classes)\n",
        "assert_equal(len(test_vals), DataProperties.num_classes)\n",
        "assert_equal(len(val_vals), DataProperties.num_classes)"
      ],
      "metadata": {
        "id": "j8tY1GFIHJ92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class balance"
      ],
      "metadata": {
        "id": "BsYj3AP1H8Rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can see, that the distribution of classes is uniform in train, validation and test sets"
      ],
      "metadata": {
        "id": "DCN9OcbGJci4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize = (16, 5))\n",
        "x_lab = 'Number of class'\n",
        "\n",
        "axes[0].set(xlabel = x_lab)\n",
        "axes[1].set(xlabel = x_lab)\n",
        "axes[2].set(xlabel = x_lab)\n",
        "\n",
        "sns.countplot(Y_train, ax = axes[0])\n",
        "sns.countplot(Y_test, ax = axes[1])\n",
        "sns.countplot(Y_val, ax = axes[2])"
      ],
      "metadata": {
        "id": "dl9mMIR8H-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image uniformity"
      ],
      "metadata": {
        "id": "h0d2A8fOJwXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here we check greyscale value distribution, to see if it depends on the class. This is done for further data processing, e.g. standartization\n",
        "- Conclusions:\n",
        "    - Min = 0, Max = 255 (Greyscale value)\n",
        "    - Distribution of greyscale value is bimodal. Two modes:\n",
        "        - ~0\n",
        "        - ~220-230\n",
        "    - Big difference in modes' amplitude -> it is reasonable to preprocess images with standartization / scaling"
      ],
      "metadata": {
        "id": "FgYO6lmsT_UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_pixel_dist(X_train, \n",
        "                   Y_train,\n",
        "                   classes = DataProperties.classes,\n",
        "                   sample_size = 150\n",
        ")"
      ],
      "metadata": {
        "id": "MMXdveiuUPdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image visualization"
      ],
      "metadata": {
        "id": "KMxYb5EWMpC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Visualize images of clothes, related to class numbers from 0 to 9\n",
        "- Conclusions:\n",
        "        0: 'T-shirt',\n",
        "        1: 'Trouser', \n",
        "        2: 'Pullover',\n",
        "        3: 'Dress',\n",
        "        4: 'Coat',\n",
        "        5: 'Sandal', \n",
        "        6: 'Shirt',\n",
        "        7: 'Sneaker',\n",
        "        8: 'Bag',\n",
        "        9: 'Ankle boot'"
      ],
      "metadata": {
        "id": "0R6cGYMtPBlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for class_n in range(10):\n",
        "    plot_class(\n",
        "        class_n,\n",
        "        X = X_train,\n",
        "        Y = Y_train,\n",
        "        figsize = (10, 5),\n",
        "        matsize = (2, 3)\n",
        "    )"
      ],
      "metadata": {
        "id": "4_M3NLJ5Msul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "TCs6GdJ3ZhlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Conclusions:\n",
        "    - Clean dataset (No NAN values)\n",
        "    - Images as input (No subdivision on categorical, data -> no preprocessing with encoding)\n",
        "    - No color channel (Gray images, already flattened)\n",
        "- After applying preprocessing, we can see:\n",
        "    - How it changes distribution of pixels brightness\n",
        "    - How it affects visual perception of the image"
      ],
      "metadata": {
        "id": "-73QNxtsa3Wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "LM7HSSgJdHpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment with HOG transform"
      ],
      "metadata": {
        "id": "_xOEk3BIE5UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we demonstrate the effect of HOG transformation. Because of small size of images, HOG preprocessing does not give better effect"
      ],
      "metadata": {
        "id": "Uf3UkhTlE748"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.feature import hog\n",
        "from skimage import data, exposure\n",
        "\n",
        "\n",
        "image = np.array(X_train[1]).reshape(28, 28)\n",
        "\n",
        "fd, hog_image = hog(\n",
        "    image, \n",
        "    orientations=8, \n",
        "    pixels_per_cell=(4, 4),\n",
        "    cells_per_block=(3, 3), \n",
        "    visualize=True\n",
        ")\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
        "\n",
        "ax1.axis('off')\n",
        "ax1.imshow(image, cmap=plt.cm.gray)\n",
        "ax1.set_title('Input image')\n",
        "\n",
        "# Rescale histogram for better display\n",
        "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
        "\n",
        "ax2.axis('off')\n",
        "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
        "ax2.set_title('Histogram of Oriented Gradients')\n",
        "plt.show()\n",
        "print(hog_image.shape, hog_image_rescaled.shape)"
      ],
      "metadata": {
        "id": "uORANpOXBeA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipeline"
      ],
      "metadata": {
        "id": "olQgXacZFL1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preproc_pipeline = Pipeline(\n",
        "    [\n",
        "        ('reshape', Reshaper()),\n",
        "        \n",
        "        # Sobel edge highlight\n",
        "        ('highlight edges', EdgeHighlighter(add_coef = 50)),  \n",
        "     \n",
        "        #('hog', HogTransformer()), # Did not work well\n",
        "        ('flatten', Flatter()),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train = preproc_pipeline.fit_transform(X_train)\n",
        "X_val = preproc_pipeline.transform(X_val)\n",
        "X_test = preproc_pipeline.transform(X_test)"
      ],
      "metadata": {
        "id": "qr5jU89lZjou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization after preprocessing"
      ],
      "metadata": {
        "id": "yYxBpytbdKCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution changed after preprocessing\n",
        "compare_pixel_dist(X_train, \n",
        "                   Y_train,\n",
        "                   classes = DataProperties.classes,\n",
        "                   sample_size = 150\n",
        ")"
      ],
      "metadata": {
        "id": "qHVdfT1hcSaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for class_n in range(10):\n",
        "    plot_class(\n",
        "        class_n,\n",
        "        X = X_train,\n",
        "        Y = Y_train,\n",
        "        figsize = (10, 5),\n",
        "        matsize = (2, 3)\n",
        "    )"
      ],
      "metadata": {
        "id": "sgdR0j_scpqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "CvKO8woTmzOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To avoid redundancy, all search procedures and parameter grids are encapsulated inside classes, inherited from GSModel (Grid Search Model) class\n",
        "- Resulting models will be compared based on several metrics for multiclass classification:\n",
        "    - Accuracy\n",
        "    - ROC-AUC\n",
        "    - F1 \n",
        "    - Precision & Recall\n",
        "    - Confusion matrix analysis\n",
        "- In the later part, we show that F1, Accuracy, Precision and Recall are almost identical in values. Therefore, any of these metrics can be applied on gridsearch CV\n",
        "- Cross validation parameters for compared models are kept identical (cv = 5)"
      ],
      "metadata": {
        "id": "mS_xxTHMlaj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models Training (Cross Validation)"
      ],
      "metadata": {
        "id": "tNC6lCZxQOhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorers = {\n",
        "    'Accuracy': make_scorer(accuracy_score),\n",
        "    'F1': 'f1_macro'\n",
        "}\n",
        "\n",
        "models = [\n",
        "    # XGBModel()  # Extremely long\n",
        "    RandomForestModel(),  # Random forest\n",
        "    SVMModel(),  # SVM\n",
        "    DTModel()  # Decision tree\n",
        "]\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "for model in models:\n",
        "    clf = GridSearchCV(\n",
        "        model.clf,\n",
        "        model.get_grid(),\n",
        "        scoring = scorers,  # Several metrics to track\n",
        "        refit = 'F1',  # Score used to find best parameters (among several metrics)\n",
        "        return_train_score = True,  # Explore train behaviour\n",
        "        **model.get_search_parameters()\n",
        "    )\n",
        "    clf.fit(X_train, Y_train)\n",
        "    cv_results.update({model.name: clf})"
      ],
      "metadata": {
        "id": "QlJUUDadm1ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model comparison"
      ],
      "metadata": {
        "id": "IuNmMuIFIEBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best score comparison"
      ],
      "metadata": {
        "id": "Ab5qvAejQrxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_scores = get_best_scores(cv_results)\n",
        "plot_best_scores(\n",
        "    best_scores,\n",
        "    y_label = 'Score (max = 1.0)',\n",
        "    title = 'Best scores on cross validation (F1 macro)'\n",
        ")"
      ],
      "metadata": {
        "id": "uv29lGV-Qvkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average CV train & CV test performance"
      ],
      "metadata": {
        "id": "xRioqPlAYpgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cv_metrics(\n",
        "    cv_results,\n",
        "    metrics_name = 'Accuracy',\n",
        "    figsize = (15, 5),\n",
        "    layout_shape = (1, 3)\n",
        ")"
      ],
      "metadata": {
        "id": "3KZY6fqzZNtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cv_metrics(\n",
        "    cv_results,\n",
        "    metrics_name = 'F1',\n",
        "    figsize = (15, 5),\n",
        "    layout_shape = (1, 3)\n",
        ")"
      ],
      "metadata": {
        "id": "qOwwFQHCtNUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning curves for best estimators, comparison"
      ],
      "metadata": {
        "id": "bFz1sJ7DiHjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we plot learning curves for chosen models. Since plotting learning curves requires models re-fit on several subsets of data (e.g. 5%, 10%, ... 95%, 100%), it is the most time-consuming operation.\n",
        "Therefore, the presented learning curves **correspond to a smaller subset of training data**"
      ],
      "metadata": {
        "id": "hbCVrJ--3prx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, res in cv_results.items():\n",
        "    model = res.best_estimator_\n",
        "    plot_learning_curve(\n",
        "        model = res.best_estimator_,\n",
        "        model_name = model_name,\n",
        "        X = X_train[: DataProperties.learning_curve_subset_size],\n",
        "        Y = Y_train[: DataProperties.learning_curve_subset_size],\n",
        "        train_sizes = [0.1, 0.25, 0.4, 0.65, 0.9, 1.0],\n",
        "        cv = 4,\n",
        "        scoring = 'accuracy',\n",
        "        metrics_name = 'Accuracy'\n",
        "    )"
      ],
      "metadata": {
        "id": "lyDytfngzXjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix analysis"
      ],
      "metadata": {
        "id": "jwlDWQ2PjkqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Models confusion matrices"
      ],
      "metadata": {
        "id": "kFd-rTDCHCKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we make several important conclusions:\n",
        "- Weakly-learned models mistake one type of clothes for another. For example, for a train set of 200 objects (purposely weakly-learned models), mistakenly classified models:\n",
        "    - Predicted: T-shirt, True: Dress (15%)\n",
        "    - Predicted: T-shirt, True: Shirt (26%)\n",
        "    - Predicted: Coat, True: Pullover (28%)\n",
        "    - Predicted: Sneaker, True: Sandal (17%)\n",
        "This is an important insight. For a real task, an additional data (e.g. about materials used (leather, fluff, cotton)) will help to distinguish between **winter** and **summer** clothes.\n"
      ],
      "metadata": {
        "id": "Ey-t_DTWvnn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, model in cv_results.items():\n",
        "    plot_confusion_matrix(\n",
        "        model,\n",
        "        model_name,\n",
        "        X_test,\n",
        "        Y_test, \n",
        "        figsize = (6, 6)\n",
        "    )"
      ],
      "metadata": {
        "id": "S_mNbiQ9joGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examples of mistaken images"
      ],
      "metadata": {
        "id": "q4M4wX-GHG4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Below a subset of classification mistakes is presented. Each class from 0 to 9, we consider as a true label. Then, look for **top_n** mistaken labels. Hence, we cover all possible pairs in confusion matrix with the highest mistake rate.\n",
        "- Analysis of mistaken images provides several important insights. Thus, the most mistaken class pairs are:\n",
        "    - T-shirt & Shirt\n",
        "    - Pullover & Coat\n",
        "    - Pullover & Dress\n",
        "    - Sneaker & Ankle boot\n",
        "    - Sandal & Ankle boot\n",
        "    - etc.\n",
        "- We see how clothes of similar look can be easily misclassified. If it was a commercial task, an additional dataset on materials used (#, fluff, cotton, leather) will help to reduce mistakes."
      ],
      "metadata": {
        "id": "aXTO_bWLG4Q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SVM model"
      ],
      "metadata": {
        "id": "iOO7ANGxdxtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'SVM'\n",
        "\n",
        "plot_mistakes(\n",
        "    best_estimator = cv_results[name].best_estimator_,\n",
        "    model_name = name,\n",
        "    X = X_test,\n",
        "    Y_true = Y_test,\n",
        "    Y_predicted = cv_results[name].best_estimator_.predict(X_test),\n",
        "    top_n = 2  # Show top_n mistaken classes\n",
        ")"
      ],
      "metadata": {
        "id": "DapRD0sfZsT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Random forest model"
      ],
      "metadata": {
        "id": "QVwUjc0wd3bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'RandomForest'\n",
        "\n",
        "plot_mistakes(\n",
        "    best_estimator = cv_results[name].best_estimator_,\n",
        "    model_name = name,\n",
        "    X = X_test,\n",
        "    Y_true = Y_test,\n",
        "    Y_predicted = cv_results[name].best_estimator_.predict(X_test),\n",
        "    top_n = 2  # Show top_n mistaken classes\n",
        ")"
      ],
      "metadata": {
        "id": "q2-UKwDXIHTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Decision tree model"
      ],
      "metadata": {
        "id": "WoiPAeudeKzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'DecisionTree'\n",
        "\n",
        "plot_mistakes(\n",
        "    best_estimator = cv_results[name].best_estimator_,\n",
        "    model_name = name,\n",
        "    X = X_test,\n",
        "    Y_true = Y_test,\n",
        "    Y_predicted = cv_results[name].best_estimator_.predict(X_test),\n",
        "    top_n = 2  # Show top_n mistaken classes\n",
        ")"
      ],
      "metadata": {
        "id": "0b_aOksQeOD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network approach"
      ],
      "metadata": {
        "id": "uKz5s6woH8S6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct & compile model"
      ],
      "metadata": {
        "id": "cVMB2CwRMvYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NNModel, stopper = construct_nn_model(\n",
        "    input_dimension = X_train.shape[1],\n",
        "    learning_rate = 0.002\n",
        ")\n",
        "\n",
        "plot_model(NNModel, show_shapes = True)"
      ],
      "metadata": {
        "id": "LYTerSEqAADs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode target values"
      ],
      "metadata": {
        "id": "TCKgxG_oMzrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_nn = to_categorical(Y_train, num_classes = DataProperties.num_classes)\n",
        "Y_test_nn = to_categorical(Y_test, num_classes = DataProperties.num_classes)"
      ],
      "metadata": {
        "id": "6H6oy7S1BUQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "8tKjGRNgM6DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = NNModel.fit(\n",
        "    X_train,\n",
        "    Y_train_nn,\n",
        "    epochs = 500,\n",
        "    batch_size = 16,\n",
        "    verbose = 1,\n",
        "    validation_split = 0.2,\n",
        "    callbacks = [stopper]\n",
        ")"
      ],
      "metadata": {
        "id": "b6MD51lHBOOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot learning curve"
      ],
      "metadata": {
        "id": "-xLzjhJXNAPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_nn_learning_curve(\n",
        "    history.history,\n",
        "    metrics_name = 'accuracy',\n",
        "    val_metrics_name = 'val_accuracy'\n",
        ")\n",
        "\n",
        "plot_nn_learning_curve(\n",
        "    history.history,\n",
        "    metrics_name = 'loss',\n",
        "    val_metrics_name = 'val_loss'\n",
        ")"
      ],
      "metadata": {
        "id": "_kwyzlU8NC4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose best model on validation set"
      ],
      "metadata": {
        "id": "nYA0KtmSNsdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In GridSearchCV we compared all models except for neural network. In the final step, we will compare best models with Neural Network, based on several metrics\n",
        "- Accuracy (valid, since data is balanced)\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 score"
      ],
      "metadata": {
        "id": "oTkLPRbbQJYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_estimators_dict = {\n",
        "    name: item.best_estimator_ for name, item in cv_results.items()\n",
        "}\n",
        "best_estimators_dict['NeuralNetwork'] = NNModel\n",
        "\n",
        "precisions, recalls, accuracies, F1 = collect_classif_reports(\n",
        "    best_estimators_dict,\n",
        "    X = X_val,\n",
        "    Y = Y_val\n",
        ")"
      ],
      "metadata": {
        "id": "nO_aKPyRQrC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_metrics = pd.DataFrame.from_dict(precisions, orient='index', columns = ['Precision'])\n",
        "final_metrics['F1'] = F1.values()\n",
        "final_metrics['Recall'] = recalls.values()\n",
        "final_metrics['Accuracy'] = accuracies.values()\n",
        "\n",
        "ax = final_metrics.plot.bar(\n",
        "    rot = 0,\n",
        "    xlabel = 'Best model',\n",
        "    ylabel = 'Metrics results',\n",
        "    cmap = 'Paired',\n",
        "    title = 'Validation performance of best models'\n",
        ")\n",
        "ax.legend(loc = 3)"
      ],
      "metadata": {
        "id": "p-eQ5clGIr_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best model test performance"
      ],
      "metadata": {
        "id": "L9VXEifJxy21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Refit on train + validation \n",
        "- Report final results on unseen test set"
      ],
      "metadata": {
        "id": "SGZTNbmh24en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#final_model = NNModel\n",
        "X_final = np.vstack([X_train, X_val])\n",
        "Y_final = np.concatenate([Y_train, Y_val])\n",
        "X_final.shape, Y_final.shape"
      ],
      "metadata": {
        "id": "iVnxS9G83CTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = cv_results['SVM'].best_estimator_ \n",
        "\n",
        "best_model.fit(\n",
        "    X_final,\n",
        "    Y_final\n",
        ")"
      ],
      "metadata": {
        "id": "yPVd1DAIMaCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fin_precisions, fin_recalls, fin_accuracies, fin_F1 = collect_classif_reports(\n",
        "    {'SVM': best_model},\n",
        "    X = X_test,\n",
        "    Y = Y_test\n",
        ")\n",
        "print(fin_precisions, fin_recalls, fin_accuracies, fin_F1)\n",
        "\n",
        "final_metrics = pd.DataFrame.from_dict(fin_precisions, orient='index', columns = ['Precision'])\n",
        "final_metrics['F1'] = fin_F1.values()\n",
        "final_metrics['Recall'] = fin_recalls.values()\n",
        "final_metrics['Accuracy'] = fin_accuracies.values()\n",
        "\n",
        "ax = final_metrics.plot.bar(\n",
        "    rot = 0,\n",
        "    xlabel = 'Best model',\n",
        "    ylabel = 'Metrics results',\n",
        "    cmap = 'Paired',\n",
        "    title = 'Test performance of best model'\n",
        ")\n",
        "ax.legend(loc = 3)"
      ],
      "metadata": {
        "id": "6WHPJcnmM6oo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}